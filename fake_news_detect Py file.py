# -*- coding: utf-8 -*-
"""Fake_News_Detect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KEkpKte-Rbdy3xjd8MFwH_b9txqDue6Q
"""

!pip install gensim

import gensim

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from wordcloud import WordCloud

!pip install tensorflow

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

"""FAKE NEWS DATASET ANALYSIS"""

fake_news = pd.read_csv("/content/fake.csv")

fake_news.head()

fake_news.columns

fake_news['text'][0]

# get the average number of words per text:
average_word_count = fake_news['text'].apply(lambda x: len(str(x).split())).mean()
print(f'Average number of words per text: {average_word_count:.2f}')

fake_news['subject'].value_counts()

# pie chart to visualize the distrubution of the subjects in the dataset
subject_counts = fake_news['subject'].value_counts()
plt.figure(figsize=(8,8))
plt.pie(subject_counts, labels=subject_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Subjects')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10,6))
sns.countplot(x='subject', data = fake_news)

# term frequency bar plot  - use a horizontal or vertical bar plot of the top N most frequent word
from collections import Counter

words = ' '.join(fake_news['text']).split()
common_words = Counter(words).most_common(20)
words_df = pd.DataFrame(common_words, columns=['word', 'count'])

sns.barplot(data=words_df, y='word', x='count')

# check for any empty rows in the fake news dataset:
def find_empty_rows(df, text_column='text'):
    # Find empty or whitespace-only or null rows
    empty_rows = df[df[text_column].str.strip().eq('') | df[text_column].isnull()]

    if not empty_rows.empty:
        print(f"Empty text rows found at indices: {list(empty_rows.index)}")
    else:
        print("No empty text rows found.")

    return empty_rows

def remove_empty_rows(df, text_column='text'):
    # Find rows where the specified column is empty or contains only whitespace
    empty_rows = df[df[text_column].str.strip().eq('') | df[text_column].isnull()]

    if not empty_rows.empty:
        print(f"Dropping the following empty row indices: {list(empty_rows.index)}")
        df = df.drop(empty_rows.index, axis=0)
    else:
        print("No empty rows found.")

    return df

find_empty_rows(fake_news, text_column='text')

remove_empty_rows(fake_news,'text')

fake_news['text'][10923]

"""Analysis of the real news dataset:"""

real_news = pd.read_csv('/content/true.csv')

real_news.head()

real_news['text'][0]

# get the average number of words per text:
average_word_count = real_news['text'].apply(lambda x: len(str(x).split())).mean()
print(f'Average number of words per text: {average_word_count:.2f}')

text = ' '.join(real_news['text'].tolist())

real_news['subject'].value_counts()

#Pie chart to visualize the distribution of the news
subject_counts = real_news['subject'].value_counts()
plt.figure(figsize=(8,8))
plt.pie(subject_counts, labels=subject_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Subjects in real news')
plt.axis('equal')  # Equal aspect ratio makes the pie a circle
plt.show()

words = ' '.join(real_news['text']).split()
common_words = Counter(words).most_common(20)
words_df = pd.DataFrame(common_words, columns=['word', 'count'])

sns.barplot(data=words_df, y='word', x='count')

real_news.sample(5)

# create list of indexes which do not have publication information by breaking text with hyphen
import re

pattern = re.compile(r'^.{0,119} - .+')

unknown_publishers = [
    i for i, text in enumerate(real_news.text.values)
    if not pattern.match(text)
]

len(unknown_publishers)

real_news.iloc[unknown_publishers].text

find_empty_rows(real_news,'text')

real_news =remove_empty_rows(real_news)

publisher = []
tmp_text = [] # for unknown publishers

for index, row in enumerate(real_news.text.values):
  if index in unknown_publishers:
    tmp_text.append(row)
    publisher.append('Unknown')
    continue
  else:
    record = row.split('-', maxsplit = 1)
    publisher.append(record[0].strip())
    tmp_text.append(record[1].strip())

real_news['publisher'] = publisher
real_news['text'] = tmp_text

real_news.shape

publisher

real_news['text'] = real_news['title'] + " " + real_news['text']
fake_news['text'] = fake_news['title'] + " " + fake_news['text']

real_news['text']  = real_news['text'].apply(lambda x: str(x).lower())
fake_news['text'] = fake_news['text'].apply(lambda x: str(x).lower())

real_news['class'] = 1
fake_news['class'] = 0

real_news = real_news[['text','class']]

fake_news = fake_news[['text', 'class']]

data = pd.concat([real_news, fake_news], ignore_index=True)

data.sample(20)

"""PREPROCESSING THE DATA"""

import re
import unicodedata
import pandas as pd

def clean_text(text):

    text = text.lower() # convert all text to lowercase
    text = text.replace('\\', '') # remove all backslashes
    text = text.replace('-', ' ') # remove any additional hyphens
    text = re.sub(r'\S+@\S+', '', text) #remove emails
    text = re.sub(r'@\w+', '', text)  #remove twitter handle
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) #remove urls
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')  # Remove accented characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) # Remove special characters and punctuation (keep only letters and numbers)
    text = re.sub(r'\s+', ' ', text).strip()    # Remove extra spaces

    return text

data['text'] = data['text'].apply(clean_text)

data['text'].head()

"""using a vectorization technique such as Word2Vec to vectorize the data"""

y = data['class'].values

X=[d.split() for d in data['text'].tolist()]

print(X[0])

DIM = 100
w2v_model = gensim.models.Word2Vec(sentences=X, vector_size=DIM, window=10, min_count=1)

len(w2v_model.wv.key_to_index)

w2v_model.wv.most_similar('weather')

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

nos = np.fromiter((len(seq) for seq in X), dtype=int)
count = np.sum(nos > 1000)
print(count)

maxlen = 1000
X = pad_sequences(X, maxlen=maxlen)

len(X[101])

vocab_size = len(tokenizer.word_index) + 1

def create_embedding_matrix(w2v_model, tokenizer, embedding_dim, vocab_size):
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, idx in tokenizer.word_index.items():
        if word in w2v_model.wv:
            embedding_vector = w2v_model.wv[word]
            embedding_matrix[idx] = embedding_vector

    return embedding_matrix

embedding_vectors = create_embedding_matrix(w2v_model, tokenizer, 100, vocab_size)

embedding_vectors.shape

"""MODEL

"""

from keras.layers import Dropout

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=DIM,
                    weights=[embedding_vectors],
                    trainable=False))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()

X_train, X_test, y_train, y_test = train_test_split(X,y)

history = model.fit(X_train, y_train,
                    validation_split=0.3,
                    epochs=5,
                    batch_size=32,
                    verbose=1)

model.save('lstm_with_dropout.h5')

# Plot Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['acc'], label='Train Accuracy')
plt.plot(history.history['val_acc'], label='Validation Accuracy')
plt.title( 'LSTM Validation vs training graph')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs for LSTM')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
import numpy as np

#Predict probabilities on test data
y_probs = model.predict(X_test)

# Convert probabilities to binary predictions
y_pred = (y_probs > 0.5).astype(int)


print(classification_report(y_test, y_pred))

# calculate precision, f1 and recall
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

#Predict and convert to binary
y_probs = model.predict(X_test)
y_pred = (y_probs > 0.5).astype(int)

#confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot with seaborn heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix For the LSTM')
plt.show()

# check with real news from internet (verified)
sample = ["Trump teases possible US strike as Iran supreme leader warns America"]
seq = tokenizer.texts_to_sequences(sample)
padded = pad_sequences(seq, maxlen=100)
print(model.predict(padded))
label = "Real News" if model.predict(padded)[0][0] > 0.5 else "Fake News"
print(label)

loss, acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {acc:.4f}")
print(f"Test Loss: {loss:.4f}")

import pickle
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)